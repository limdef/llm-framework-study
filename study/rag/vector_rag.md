# RAG

가지고 있는 문서를 활용해서 LLM 모델이 답변하도록 하는 기법

1. 문서 쪼개기  
 문서를 작은 단위(문단, 문장 등)로 나눔 (chunking)

 2. 임베딩 & 저장  
 각 조각을 벡터로 임베딩해서 벡터 DB에 저장  
3. 질문 임베딩 & 검색  
* 사용자의 질문도 벡터로 변환  
* 벡터 DB에서 유사한 문서 조각들을 검색 (코사인 유사도)
4. 문서 + 질문 → LLM에게 전달  
* 검색된 문서들을 LLM에 함께 전달해서  
* 문맥 기반으로 더 정확한 답변 생성


## llama-index

위 RAG의 공식화된 과정을 매번 구현해야 하니까 추상화해서 개발자가 편하게 구현하도록 하는 라이브러리

* 임베딩 모델 : 문서를 벡터로 바꿔주는 모델
ex. OpenAIEmbedding(model="text-embedding-3-small")



### 벡터 DB

오픈 소스로 이미 많이 구현되어 있음.

|이름 |	특징 |	장점 |	단점 |
| --- | ---- | ---- | --- |
|FAISS (Meta) | 오픈소스, 성능 우수|	빠르고, 로컬에서 사용 쉬움 | 디스크 저장/운영 기능 부족|
|Chroma|	LlamaIndex & LangChain 친화적 |	설치 쉽고, 빠르게 시작 가능	|대규모 운영은 아직 미흡|
|Weaviate|	오픈소스 + 클라우드 지원 |	필터링, GraphQL, 멀티모달 지원	|설정이 약간 복잡|
|Pinecone|	상용 서비스	| 안정적인 클라우드 벡터 검색 |	유료 (무료 티어 있음)|
|Qdrant|	러스트 기반, 빠르고 신뢰성 높음	|필터링, 클러스터링 등 잘됨	|처음 설치 시 러닝커브 약간 있음 |
|Milvus|	대규모, 고성능	| 하이엔드 기업형 시스템에 적합	| 무겁고 세팅 복잡|
